diff --git a/vendor/knative.dev/eventing/pkg/scheduler/state/helpers.go b/vendor/knative.dev/eventing/pkg/scheduler/state/helpers.go
index 56ca82b8d..b37fc7997 100644
--- a/vendor/knative.dev/eventing/pkg/scheduler/state/helpers.go
+++ b/vendor/knative.dev/eventing/pkg/scheduler/state/helpers.go
@@ -20,10 +20,8 @@ import (
 	"math"
 	"strconv"
 	"strings"
-	"time"
 
 	"k8s.io/apimachinery/pkg/types"
-	"k8s.io/apimachinery/pkg/util/wait"
 	"knative.dev/eventing/pkg/scheduler"
 )
 
@@ -51,13 +49,8 @@ func GetVPod(key types.NamespacedName, vpods []scheduler.VPod) scheduler.VPod {
 
 func SatisfyZoneAvailability(feasiblePods []int32, states *State) bool {
 	zoneMap := make(map[string]struct{})
-	var zoneName string
-	var err error
 	for _, podID := range feasiblePods {
-		wait.PollImmediate(50*time.Millisecond, 5*time.Second, func() (bool, error) {
-			zoneName, _, err = states.GetPodInfo(PodNameFromOrdinal(states.StatefulSetName, podID))
-			return err == nil, nil
-		})
+		zoneName, _, _ := states.GetPodInfo(PodNameFromOrdinal(states.StatefulSetName, podID))
 		zoneMap[zoneName] = struct{}{}
 	}
 	return len(zoneMap) == int(states.NumZones)
@@ -65,13 +58,8 @@ func SatisfyZoneAvailability(feasiblePods []int32, states *State) bool {
 
 func SatisfyNodeAvailability(feasiblePods []int32, states *State) bool {
 	nodeMap := make(map[string]struct{})
-	var nodeName string
-	var err error
 	for _, podID := range feasiblePods {
-		wait.PollImmediate(50*time.Millisecond, 5*time.Second, func() (bool, error) {
-			_, nodeName, err = states.GetPodInfo(PodNameFromOrdinal(states.StatefulSetName, podID))
-			return err == nil, nil
-		})
+		_, nodeName, _ := states.GetPodInfo(PodNameFromOrdinal(states.StatefulSetName, podID))
 		nodeMap[nodeName] = struct{}{}
 	}
 	return len(nodeMap) == int(states.NumNodes)
diff --git a/vendor/knative.dev/eventing/pkg/scheduler/state/state.go b/vendor/knative.dev/eventing/pkg/scheduler/state/state.go
index 38eaa5e68..e84f311fc 100644
--- a/vendor/knative.dev/eventing/pkg/scheduler/state/state.go
+++ b/vendor/knative.dev/eventing/pkg/scheduler/state/state.go
@@ -21,7 +21,6 @@ import (
 	"errors"
 	"fmt"
 	"strconv"
-	"time"
 
 	"go.uber.org/zap"
 	v1 "k8s.io/api/core/v1"
@@ -29,13 +28,14 @@ import (
 	"k8s.io/apimachinery/pkg/labels"
 	"k8s.io/apimachinery/pkg/types"
 	"k8s.io/apimachinery/pkg/util/sets"
-	"k8s.io/apimachinery/pkg/util/wait"
 	clientappsv1 "k8s.io/client-go/kubernetes/typed/apps/v1"
 	corev1 "k8s.io/client-go/listers/core/v1"
 
 	kubeclient "knative.dev/pkg/client/injection/kube/client"
 	"knative.dev/pkg/logging"
 
+	apierrors "k8s.io/apimachinery/pkg/api/errors"
+
 	"knative.dev/eventing/pkg/scheduler"
 )
 
@@ -218,11 +218,11 @@ func (s *stateBuilder) State(reserved map[types.NamespacedName]map[string]int32)
 	}
 
 	for podId := int32(0); podId < scale.Spec.Replicas && s.podLister != nil; podId++ {
-		var pod *v1.Pod
-		wait.PollImmediate(50*time.Millisecond, 5*time.Second, func() (bool, error) {
-			pod, err = s.podLister.Get(PodNameFromOrdinal(s.statefulSetName, podId))
-			return err == nil, nil
-		})
+		podName := PodNameFromOrdinal(s.statefulSetName, podId)
+		pod, err := s.podLister.Get(podName)
+		if err != nil && !apierrors.IsNotFound(err) {
+			return nil, fmt.Errorf("failed to get pod %s from lister: %w", podName, err)
+		}
 
 		if pod != nil {
 			if isPodUnschedulable(pod) {
@@ -266,11 +266,10 @@ func (s *stateBuilder) State(reserved map[types.NamespacedName]map[string]int32)
 
 			withPlacement[vpod.GetKey()][podName] = true
 
-			var pod *v1.Pod
-			wait.PollImmediate(50*time.Millisecond, 5*time.Second, func() (bool, error) {
-				pod, err = s.podLister.Get(podName)
-				return err == nil, nil
-			})
+			pod, err := s.podLister.Get(podName)
+			if err != nil && !apierrors.IsNotFound(err) {
+				return nil, fmt.Errorf("failed to get pod %s from lister: %w", podName, err)
+			}
 
 			if pod != nil && schedulablePods.Has(OrdinalFromPodName(pod.GetName())) {
 				nodeName := pod.Spec.NodeName       //node name for this pod
@@ -291,11 +290,10 @@ func (s *stateBuilder) State(reserved map[types.NamespacedName]map[string]int32)
 					continue
 				}
 
-				var pod *v1.Pod
-				wait.PollImmediate(50*time.Millisecond, 5*time.Second, func() (bool, error) {
-					pod, err = s.podLister.Get(podName)
-					return err == nil, nil
-				})
+				pod, err := s.podLister.Get(podName)
+				if err != nil && !apierrors.IsNotFound(err) {
+					return nil, fmt.Errorf("failed to get pod %s from pod lister: %w", podName, err)
+				}
 
 				if pod != nil && schedulablePods.Has(OrdinalFromPodName(pod.GetName())) {
 					nodeName := pod.Spec.NodeName       //node name for this pod
diff --git a/vendor/knative.dev/eventing/pkg/scheduler/statefulset/autoscaler.go b/vendor/knative.dev/eventing/pkg/scheduler/statefulset/autoscaler.go
index 6b66a0584..d0a54f208 100644
--- a/vendor/knative.dev/eventing/pkg/scheduler/statefulset/autoscaler.go
+++ b/vendor/knative.dev/eventing/pkg/scheduler/statefulset/autoscaler.go
@@ -18,12 +18,14 @@ package statefulset
 
 import (
 	"context"
+	"fmt"
 	"math"
 	"sync"
 	"time"
 
 	"go.uber.org/zap"
 	v1 "k8s.io/api/core/v1"
+	apierrors "k8s.io/apimachinery/pkg/api/errors"
 	metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
 	"k8s.io/apimachinery/pkg/util/wait"
 	clientappsv1 "k8s.io/client-go/kubernetes/typed/apps/v1"
@@ -43,13 +45,18 @@ type Autoscaler interface {
 	Autoscale(ctx context.Context, attemptScaleDown bool, pending int32)
 }
 
+type AutoscaleTrigger struct {
+	AttempScaleDown bool
+	Pending         int32
+}
+
 type autoscaler struct {
 	statefulSetClient clientappsv1.StatefulSetInterface
 	statefulSetName   string
 	vpodLister        scheduler.VPodLister
 	logger            *zap.SugaredLogger
 	stateAccessor     st.StateAccessor
-	trigger           chan int32
+	trigger           chan AutoscaleTrigger
 	evictor           scheduler.Evictor
 
 	// capacity is the total number of virtual replicas available per pod.
@@ -75,7 +82,7 @@ func NewAutoscaler(ctx context.Context,
 		vpodLister:        lister,
 		stateAccessor:     stateAccessor,
 		evictor:           evictor,
-		trigger:           make(chan int32, 1),
+		trigger:           make(chan AutoscaleTrigger, 20),
 		capacity:          capacity,
 		refreshPeriod:     refreshPeriod,
 		lock:              new(sync.Mutex),
@@ -83,27 +90,33 @@ func NewAutoscaler(ctx context.Context,
 }
 
 func (a *autoscaler) Start(ctx context.Context) {
-	attemptScaleDown := false
-	pending := int32(0)
+	a.maybeTriggerAutoscaleOnStart()
 	for {
 		select {
 		case <-ctx.Done():
 			return
-		case <-time.After(a.refreshPeriod):
-			attemptScaleDown = true
-		case pending = <-a.trigger:
-			attemptScaleDown = false
+		case t := <-a.trigger:
+			a.syncAutoscale(ctx, t.AttempScaleDown, t.Pending)
 		}
+	}
+}
 
-		// Retry a few times, just so that we don't have to wait for the next beat when
-		// a transient error occurs
-		a.syncAutoscale(ctx, attemptScaleDown, pending)
-		pending = int32(0)
+// maybeTriggerAutoscaleOnStart triggers the autoscaler when it is started when the vpodLister
+// returns no vpods.
+func (a *autoscaler) maybeTriggerAutoscaleOnStart() {
+	_, err := a.vpodLister()
+	if err != nil {
+		a.logger.Warnw("failed to list vpods", "err", err)
+	} else {
+		a.trigger <- AutoscaleTrigger{AttempScaleDown: true, Pending: 0}
 	}
 }
 
 func (a *autoscaler) Autoscale(ctx context.Context, attemptScaleDown bool, pending int32) {
-	a.syncAutoscale(ctx, attemptScaleDown, pending)
+	select {
+	case a.trigger <- AutoscaleTrigger{AttempScaleDown: attemptScaleDown, Pending: pending}:
+	default:
+	}
 }
 
 func (a *autoscaler) syncAutoscale(ctx context.Context, attemptScaleDown bool, pending int32) {
@@ -168,6 +181,8 @@ func (a *autoscaler) doautoscale(ctx context.Context, attemptScaleDown bool, pen
 		newreplicas = scale.Spec.Replicas
 	}
 
+	a.logger.Debugf("expected replicas %d, got %d", newreplicas, scale.Spec.Replicas)
+
 	if newreplicas != scale.Spec.Replicas {
 		scale.Spec.Replicas = newreplicas
 		a.logger.Infow("updating adapter replicas", zap.Int32("replicas", scale.Spec.Replicas))
@@ -186,6 +201,13 @@ func (a *autoscaler) doautoscale(ctx context.Context, attemptScaleDown bool, pen
 }
 
 func (a *autoscaler) mayCompact(s *st.State, scaleUpFactor int32) {
+
+	a.logger.Debugw("Trying to compact and scale down",
+		zap.Int32("scaleUpFactor", scaleUpFactor),
+		zap.Any("schedulablePods", s.SchedulablePods),
+		zap.Int32("lastOrdinal", s.LastOrdinal),
+	)
+
 	// when there is only one pod there is nothing to move or number of pods is just enough!
 	if s.LastOrdinal < 1 || len(s.SchedulablePods) <= int(scaleUpFactor) {
 		return
@@ -239,16 +261,18 @@ func (a *autoscaler) compact(s *st.State, scaleUpFactor int32) error {
 				ordinal := st.OrdinalFromPodName(placements[i].PodName)
 
 				if ordinal == s.LastOrdinal-j {
-					wait.PollImmediate(50*time.Millisecond, 5*time.Second, func() (bool, error) {
-						if s.PodLister != nil {
-							pod, err = s.PodLister.Get(placements[i].PodName)
+					if s.PodLister != nil {
+						pod, err = s.PodLister.Get(placements[i].PodName)
+						if apierrors.IsNotFound(err) {
+							continue
+						}
+						if err != nil {
+							return fmt.Errorf("failed to get pod %s: %w", placements[i].PodName, err)
+						}
+						err = a.evictor(pod, vpod, &placements[i])
+						if err != nil {
+							return err
 						}
-						return err == nil, nil
-					})
-
-					err = a.evictor(pod, vpod, &placements[i])
-					if err != nil {
-						return err
 					}
 				}
 			}
diff --git a/vendor/knative.dev/eventing/pkg/scheduler/statefulset/scheduler.go b/vendor/knative.dev/eventing/pkg/scheduler/statefulset/scheduler.go
index df0dcfdf7..1d22452eb 100644
--- a/vendor/knative.dev/eventing/pkg/scheduler/statefulset/scheduler.go
+++ b/vendor/knative.dev/eventing/pkg/scheduler/statefulset/scheduler.go
@@ -166,6 +166,9 @@ func (s *StatefulSetScheduler) Schedule(vpod scheduler.VPod) ([]duckv1alpha1.Pla
 }
 
 func (s *StatefulSetScheduler) scheduleVPod(vpod scheduler.VPod) ([]duckv1alpha1.Placement, error) {
+	// Attempt to scale down (async)
+	defer s.autoscaler.Autoscale(s.ctx, true, s.pendingVReplicas())
+
 	logger := s.logger.With("key", vpod.GetKey())
 	logger.Info("scheduling")
 
@@ -240,7 +243,7 @@ func (s *StatefulSetScheduler) scheduleVPod(vpod scheduler.VPod) ([]duckv1alpha1
 
 		s.pending[vpod.GetKey()] = left
 
-		// Trigger the autoscaler
+		// Trigger the autoscaler (async)
 		if s.autoscaler != nil {
 			s.autoscaler.Autoscale(s.ctx, false, s.pendingVReplicas())
 		}
@@ -269,8 +272,10 @@ func (s *StatefulSetScheduler) rebalanceReplicasWithPolicy(vpod scheduler.VPod,
 }
 
 func (s *StatefulSetScheduler) removeReplicasWithPolicy(vpod scheduler.VPod, diff int32, placements []duckv1alpha1.Placement) []duckv1alpha1.Placement {
-	logger := s.logger.Named("remove replicas with policy")
+	logger := s.logger.With(zap.String("action", "remove replicas with policy"))
+
 	numVreps := diff
+	logger.Debug(zap.Int32("numVreps", numVreps))
 
 	for i := int32(0); i < numVreps; i++ { //deschedule one vreplica at a time
 		state, err := s.stateAccessor.State(s.reserved)
@@ -337,9 +342,11 @@ func (s *StatefulSetScheduler) removeSelectionFromPlacements(placementPodID int3
 }
 
 func (s *StatefulSetScheduler) addReplicasWithPolicy(vpod scheduler.VPod, diff int32, placements []duckv1alpha1.Placement) ([]duckv1alpha1.Placement, int32) {
-	logger := s.logger.Named("add replicas with policy")
+	logger := s.logger.With(zap.String("action", "add replicas with policy"))
 
 	numVreps := diff
+	logger.Debug(zap.Int32("numVreps", numVreps))
+
 	for i := int32(0); i < numVreps; i++ { //schedule one vreplica at a time (find most suitable pod placement satisying predicates with high score)
 		// Get the current placements state
 		state, err := s.stateAccessor.State(s.reserved)
@@ -355,6 +362,8 @@ func (s *StatefulSetScheduler) addReplicasWithPolicy(vpod scheduler.VPod, diff i
 			break               //end the iteration for all vreps since there are not pods
 		}
 
+		logger.Debug("Finding feasible pods")
+
 		feasiblePods := s.findFeasiblePods(s.ctx, state, vpod, state.SchedPolicy)
 		if len(feasiblePods) == 0 { //no pods available to schedule this vreplica
 			logger.Info("no feasible pods available to schedule this vreplica")
@@ -373,6 +382,8 @@ func (s *StatefulSetScheduler) addReplicasWithPolicy(vpod scheduler.VPod, diff i
 			continue
 		} */
 
+		logger.Debug("Prioritizing pods")
+
 		priorityList, err := s.prioritizePods(s.ctx, state, vpod, feasiblePods, state.SchedPolicy)
 		if err != nil {
 			logger.Info("error while scoring pods using priorities", zap.Error(err))
